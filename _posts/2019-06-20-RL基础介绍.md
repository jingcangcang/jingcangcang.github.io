---
layout:     post
title:      "RL基础介绍"
subtitle:   ""
date:       2019-06-20 11:00:00
author:     "CC"
header-img: "img/post-bg-android.jpg"
header-mask: 0.3
catalog: true 
tags:
  - RL
---


# 监督学习、无监督学习、强化学习的比较

+ 监督学习：每个输入对应一个label；

+ 无监督学习：无label，无feedback

+ 强化学习：无label，有反馈——reward system，但是反馈有延时，可能需要在几步之后才知道当前的选择是好还是坏；


# 强化学习基本知识

## 强化学习的定义

强化学习是一个sequential decision making问题，其尽量找到最优的决策序列（action序列），以使得得到的reward最大。

其在没有任何label告诉算法应该怎么做的情况下，通过先尝试做出一些行为得到一个结果，通过判断这个结果是对还是错来对之前的行为进行反馈，并根据此反馈来调整之前的行为，这样不断的尝试+调整，算法能够学习到在什么样的情况下选择什么样的行为可以得到最好的结果。

## 强化学习一般组成部分

+ history是所有状态、动作、奖赏的序列，Ht= O1,R1, A1,…,At-1,Ot,Rt
+ environment state，环境当前的状态。Agent通常不可见完整的环境状态，也就是环境的状态和环境反馈给agent的状态不一定是相同的。

  例如机器人在走路时，当前的environment状态是一个确定的位置，但是它的camera只能拍到周围的景象，无法告诉agent具体的位置，而拍摄到的照片可以认为是对环境的一个observation。
+ agent state，agent当前所处的状态，是history的函数

** environment state 和 history是有马尔可夫性质的。p(S(t+1)|S(t)) = p(S(t+1)|S(t),...,S(1)) **

---

environment 和 agent state的区别
+ Full observability

  	agent能够直接看到环境当前的状态，在这种情况下agent state与environment state是相等的；
  
  	对应markov decision process（MDP）
  
+ Partially Observable Environments

  	agent能获取到的不是直接的环境状态，agent state与environment state不等，如机器照相、打牌时只能看到公共的牌；
  
  	对应partially observable markov decision process（POMDP）

  	Agent需要构建自己的state representation，以history为基础。
  

## 强化学习中agent的组成

  一个agent由三部分组成：policy、value function、model，这三部分不是必须同时存在的。
  
  a)	policy
  
  根据agent当前看到的observation决定action，从state映射到action的映射。两种表达形式，一种是Deterministic policy即a=π(s)，在某种状态s下，一定会执行某个动作a。一种是Stochastic policy即π(a|s)=p[At=a|St=s]，它是在某种状态下执行某个动作的概率。
  
  b)	value function
  
  衡量当前状态的好坏，预测当前状态下未来可能获得的reward。
  
  c)	model
  
  预测environment下一步的动作，从而预测agent接收到的状态或reward是什么。P预测下一个state，R预测下一个reward
  
  
  + Reward（R）：用来反映在给定的时间步上，agent所做得有多好
  + Action （A）：agent在每个时间步上的决策，是reward和state的函数
  + State （S）：描述当前agent所处的环境
  + Policy（P）：一个从state映射到action的映射
  + Value function（V）：衡量每个state有多好

  ** Value function和reward区别：reward反映的是在当前状态好的程度，value则反映在长期的序列中，位于当前state好的程度 **

## 强化学习的探索(exploration)和利用(exploitation)

+ 探索(exploration)：针对POMDP情况，尝试一些新的选择，寻找更多关于environment的信息。
+ 利用(exploitation)：利用已知的信息最大化reward。

## 更新策略：回合更新（Monte-Carlo update，蒙特卡洛更新）和单步更新（Temporal-Difference update）

+ 回合更新：指游戏开始到结束整一个过程，只有等到结束后，才能够总结这个回合的所有转折点，再更新我们的行为准则。诸如Monte-carlo Learning 和 基础版的 policy gradient等。
+ 单步更新：在游戏中的每一步都可以更新其行为准则（例如如何选择action等），不用等到整个过程都结束。诸如Q learning、Sarsa 和升级版的 policy gradient 等都是单步更新的。




